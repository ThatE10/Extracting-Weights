{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6e607c-906f-4e64-befc-4cef1222f31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71df5244d6a047a9b1362791d63a8e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21e2e93-e0c5-425a-b23b-95d4015400df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2cf7ed120f4db5845b20278a3db405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, Max min gap 0.7578125, 1.6796875, 1.984375\n",
      "layer 1, Max min gap 0.9140625, 1.859375, 2.25\n",
      "layer 2, Max min gap 1.1953125, 2.15625, 2.578125\n",
      "layer 3, Max min gap 1.1953125, 2.53125, 3.125\n",
      "layer 4, Max min gap 1.65625, 2.96875, 3.671875\n",
      "layer 5, Max min gap 1.515625, 3.25, 3.9375\n",
      "layer 6, Max min gap 1.625, 3.296875, 4.0\n",
      "layer 7, Max min gap 1.65625, 3.21875, 3.921875\n",
      "layer 8, Max min gap 1.71875, 3.25, 4.03125\n",
      "layer 9, Max min gap 1.5625, 3.328125, 4.15625\n",
      "layer 10, Max min gap 1.6328125, 3.28125, 4.03125\n",
      "layer 11, Max min gap 1.6875, 3.21875, 3.96875\n",
      "layer 12, Max min gap 1.6484375, 3.140625, 3.875\n",
      "layer 13, Max min gap 1.8125, 3.328125, 4.09375\n",
      "layer 14, Max min gap 1.6328125, 3.546875, 4.375\n",
      "layer 15, Max min gap 1.75, 3.9375, 4.875\n",
      "layer 16, Max min gap 1.6796875, 4.21875, 5.28125\n",
      "layer 17, Max min gap 1.890625, 4.375, 5.5625\n",
      "layer 18, Max min gap 2.3125, 4.46875, 5.6875\n",
      "layer 19, Max min gap 1.90625, 4.46875, 5.8125\n",
      "layer 20, Max min gap 2.140625, 4.4375, 5.875\n",
      "layer 21, Max min gap 2.421875, 4.5625, 6.0\n",
      "layer 22, Max min gap 2.234375, 4.6875, 6.15625\n",
      "layer 23, Max min gap 3.0, 4.75, 6.1875\n",
      "layer 24, Max min gap 2.3125, 4.8125, 6.21875\n",
      "layer 25, Max min gap 3.359375, 4.8125, 6.0625\n",
      "layer 26, Max min gap 3.40625, 4.875, 6.0625\n",
      "layer 27, Max min gap 3.3125, 4.96875, 6.25\n",
      "layer 28, Max min gap 3.515625, 5.1875, 6.625\n",
      "layer 29, Max min gap 3.65625, 5.375, 7.0625\n",
      "layer 30, Max min gap 2.8125, 5.6875, 7.75\n",
      "layer -1, Max min gap 2.90625, 5.75, 8.3125\n",
      "Original output: According to the article, the auction was held at the Hard Rock Cafe in New York's Times Square. The auction made a total of $2 million, which is more than the expected $120,000. The Michael Jackson glove was sold for $420,000 to a buyer from Hong Kong, China, who was representing Ponte 16 Resort in Macau.\n",
      "============================\n",
      "After protection: \n",
      "============================\n",
      "--Call--\n",
      "> \u001b[32m/users/enguye17/aisb/.venv/lib/python3.11/site-packages/IPython/core/displayhook.py\u001b[39m(\u001b[92m269\u001b[39m)\u001b[36m__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    268\u001b[39m \n",
      "\u001b[32m--> 269\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, result=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[32m    270\u001b[39m         \"\"\"Printing with history cache management.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n",
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "import ipdb, copy, sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from tylor_expansion.llama import model_tylor_expansion\n",
    "# from util import model_expand, print_model_parameters, freeze_model_parameters, zero_model_parameters, ViTBase2Large\n",
    "\n",
    "dtype = \"torch.bfloat16\"\n",
    "\n",
    "\n",
    "cache_path = \"/scratch/enguye17/models/llama-3-8b\"\n",
    "os.makedirs(cache_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                                             torch_dtype=eval(dtype),\n",
    "                                             )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir=\"/scratch/enguye17/models/llama-3-8b/tokenizer\")\n",
    "\n",
    "model_expansion = copy.deepcopy(model)\n",
    "layer_clone = copy.deepcopy(model.model.layers[0].mlp)\n",
    "\n",
    "context_buf = [f\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during 'Motown 25,' an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter 'Clyde' Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. 'The legacy that [Jackson] left behind is bigger than life for me,' Orange said. 'I hope that through that glove people can see what he was trying to say in his music and what he said in his music.' Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"]\n",
    "question_buf = [ \"Where was the Auction held?\", \"How much did they make?\", \"How much did they expected?\", \"WHo buy the Jackson Glove\", \"Where was the buyer of the glove from?\" ]\n",
    "\n",
    "context = \" \".join(context_buf)\n",
    "question = \" \".join(question_buf)\n",
    "\n",
    "prompt = [\n",
    "  {\"role\": \"user\", \"content\": context}\n",
    "] + [\n",
    "  {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "# prompt = [\n",
    "#     {\"role\": \"user\", \"content\": \"Tell me a story.\"}\n",
    "# ]\n",
    "\n",
    "prompt_adapt = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_adapt, return_tensors=\"pt\")\n",
    "inputs.input_ids = inputs.input_ids.cuda()\n",
    "\n",
    "model = model.cuda()\n",
    "generate_ids1 = model.generate(inputs.input_ids, max_new_tokens=128, do_sample=False)\n",
    "outputs1 = tokenizer.batch_decode(generate_ids1[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "# print(f\"Original output: {outputs1}\")\n",
    "# print(\"============================\")\n",
    "\n",
    "select_dim = 10000\n",
    "grad_order = 6\n",
    "grad_order_min = 8\n",
    "delta_hidden_state_thd = 2.5\n",
    "expand_layer = None\n",
    "\n",
    "model_expansion = model_expansion.cuda()\n",
    "model_tylor_expansion(model_expansion,\n",
    "                      \"./output/llama-3-8b-hf-pileval-hidden-states/hidden-states-\" + dtype + \"-n-20000-len-4096.pth.tar\",\n",
    "                      select_dim=select_dim,\n",
    "                      grad_order=grad_order,\n",
    "                      expand_layer=None,\n",
    "                      grad_order_min=grad_order_min,\n",
    "                      delta_hidden_state_thd=delta_hidden_state_thd)\n",
    "generate_ids2 = model_expansion.generate(inputs.input_ids, max_new_tokens=256, do_sample=False) # do_sample=True, top_p=0.9) #\n",
    "outputs2 = tokenizer.batch_decode(generate_ids2[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "# print(f\"After protection: {outputs2}\")\n",
    "# print(\"============================\")\n",
    "print(f\"Original output: {outputs1}\")\n",
    "print(\"============================\")\n",
    "print(f\"After protection: \")\n",
    "print(\"============================\")\n",
    "\n",
    "model_tylor_log = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"dtype\": dtype,\n",
    "    \"context\": context,\n",
    "    \"question\": question,\n",
    "    \"original_output\": outputs1,\n",
    "    \"llm_protection_conifg\":\n",
    "    {\n",
    "        \"select_dim\": select_dim,\n",
    "        \"grad_order\": grad_order,\n",
    "        \"expand_layer\": expand_layer,\n",
    "        \"grad_order_min\": grad_order_min,\n",
    "        \"delta_hidden_state_thd\": delta_hidden_state_thd,\n",
    "    },\n",
    "    \"llm_protection_output\": outputs2,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "ipdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdc6fa8-c81b-457e-b966-490e2406b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 28 21:18:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:17:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             63W /  300W |   51607MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          859173      C   ...nguye17/aisb/.venv/bin/python      51598MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9eada9-e5d2-4f18-995b-72e382bb7ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): Tylor_expansion_LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a7c54b-7b58-4cb1-8e4e-dbe3aa90b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store activations per module\n",
    "\n",
    "import torch\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def activation_hook(module, input, output):\n",
    "    if module not in activations:\n",
    "        activations[module] = {\"inputs\": None, \"outputs\": None}\n",
    "\n",
    "    # --- process inputs ---\n",
    "    for i in input:\n",
    "        if isinstance(i, torch.Tensor):\n",
    "            i = i.squeeze(0)  # (seq_len, hidden_dim)\n",
    "            activations[module][\"inputs\"] = (\n",
    "                i.detach()\n",
    "                if activations[module][\"inputs\"] is None\n",
    "                else torch.cat([activations[module][\"inputs\"], i.detach()], dim=0)\n",
    "            )\n",
    "\n",
    "    # --- process outputs ---\n",
    "    if isinstance(output, tuple):\n",
    "        # If module returns tuple, just take first tensor (common in HF blocks)\n",
    "        o = output[0]\n",
    "    else:\n",
    "        o = output\n",
    "\n",
    "    o = o.squeeze(0)  # (seq_len, hidden_dim)\n",
    "    activations[module][\"outputs\"] = (\n",
    "        o.detach()\n",
    "        if activations[module][\"outputs\"] is None\n",
    "        else torch.cat([activations[module][\"outputs\"], o.detach()], dim=0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad88fa4f-8de2-43cb-bbec-b7c6c823bdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tylor_expansion_LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layer = model_expansion.model.layers[0].mlp\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49874ae5-4387-480b-82d6-1ace4ed7746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = layer.register_forward_hook(activation_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c69edf8-1daf-4550-85d7-8b2c389977a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generate_ids2 = model_expansion.generate(inputs.input_ids, max_new_tokens=256, do_sample=False) # do_sample=True, top_p=0.9) #\n",
    "outputs2 = tokenizer.batch_decode(generate_ids2[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce587333-38b8-41f1-b2b8-805a3a59e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the article, the auction was held at the Hard Rock Cafe in New York's Times Square. The auction made a total of $2 million, which is more than the expected $120,000. The Michael Jackson glove was sold for $420,000 to a buyer from Hong Kong, China, who was representing Ponte 16 Resort in Macau.\n"
     ]
    }
   ],
   "source": [
    "print(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ec39c8-db35-4d62-a796-83383b5d7750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n",
      "4096\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "#print(list(activations.values())[0]['inputs'])\n",
    "print(len(list(activations.values())[0]['inputs']))\n",
    "print(len(list(activations.values())[0]['inputs'][0]))\n",
    "print(type(list(activations.values())[0]['inputs'][0]))\n",
    "print(list(activations.values())[0]['inputs'][2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1764560d-db61-41f4-ad69-ecf6f0e2e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0166, -0.0435,  0.0245,  ...,  0.0510, -0.0181,  0.0101],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(list(activations.values())[0]['inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b694ce6d-b85b-4ed7-a071-25b2d9344828",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_buf = ['The cow goes moo :) peanut butter peanuty butter hello hello good bye meow meow moew idk']\n",
    "question_buf = [ \"Where was the Auction held?\", \"How much did they make?\", \"How much did they expected?\", \"WHo buy the Jackson Glove\", \"Where was the buyer of the glove from?\" ]\n",
    "\n",
    "context = \" \".join(context_buf)\n",
    "question = \" \".join(question_buf)\n",
    "\n",
    "prompt = [\n",
    "  {\"role\": \"user\", \"content\": context}\n",
    "] + [\n",
    "  {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "# prompt = [\n",
    "#     {\"role\": \"user\", \"content\": \"Tell me a story.\"}\n",
    "# ]\n",
    "\n",
    "prompt_adapt = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_adapt, return_tensors=\"pt\")\n",
    "inputs.input_ids = inputs.input_ids.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5adf82b3-dda9-4767-8918-1de3a2ea527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're asking about the Michael Jackson's Glove auction!\n",
      "\n",
      "The auction was held on June 24, 2019, at the Julien's Auctions in Los Angeles, California.\n",
      "\n",
      "The glove sold for a whopping $350,000!\n",
      "\n",
      "According to Julien's Auctions, the estimated value of the glove was between $100,000 to $150,000, so the final sale price was significantly higher than expected!\n",
      "\n",
      "The buyer of the glove was an anonymous bidder, but Julien's Auctions revealed that the buyer was a private collector from the United States.\n",
      "\n",
      "The glove is one of the most iconic and recognizable pieces of memorabilia from Michael Jackson's career, and it's no surprise that it fetched such a high price!\n"
     ]
    }
   ],
   "source": [
    "generate_ids2 = model_expansion.generate(inputs.input_ids, max_new_tokens=256, do_sample=False) # do_sample=True, top_p=0.9) #\n",
    "outputs2 = tokenizer.batch_decode(generate_ids2[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86defaf3-fadc-4402-89bc-710d4545f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(activations.values())[0]['inputs']\n",
    "y = list(activations.values())[0]['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c903362e-8192-486d-9db6-f53ac0834fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([692, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b19ec63-926b-4099-92da-04b3b88194e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.Size([692, 4096])\n",
      "W_gate.T:torch.Size([4096, 10000])\n",
      "W_up.T:torch.Size([4096, 10000])\n",
      "A shape: torch.Size([4096, 10000])\n",
      "torch.Size([692, 4096])\n",
      "torch.Size([692, 4096])\n",
      "Mean squared error: 0.000000\n",
      "SiLU()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation_functions = [\n",
    "    # torch.nn.ReLU(),\n",
    "    # torch.nn.LeakyReLU(),\n",
    "    # torch.nn.ELU(),\n",
    "    # torch.nn.SELU(),\n",
    "    # torch.nn.CELU(),\n",
    "    # torch.nn.GELU(),\n",
    "    torch.nn.SiLU(),     # also known as Swish\n",
    "    # torch.nn.Hardswish(),\n",
    "    # torch.nn.Softplus(),\n",
    "    # torch.nn.Softsign(),\n",
    "    # torch.nn.Tanh(),\n",
    "    # torch.nn.Tanhshrink(),\n",
    "    # torch.nn.Hardtanh(),\n",
    "    # torch.nn.Sigmoid(),\n",
    "    # torch.nn.Softshrink(),\n",
    "    # torch.nn.ReLU6(),\n",
    "    # torch.nn.LogSigmoid(),\n",
    "    # torch.nn.Softmin(dim=1),\n",
    "    # torch.nn.Softmax(dim=1),\n",
    "    # torch.nn.LogSoftmax(dim=1),\n",
    "]\n",
    "\n",
    "sample_count, n = X.shape\n",
    "print(X.dtype)\n",
    "\n",
    "for f in activation_functions:\n",
    "    \n",
    "    out = X.to(torch.bfloat16)  # cast explicitly\n",
    "    bias = torch.ones(sample_count, 1, device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    y_target = y-layer.ffn_forward(X)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(f\"W_gate.T:{layer.gate_proj_weight.T.shape}\")\n",
    "    print(f\"W_up.T:{layer.up_proj_weight.T.shape}\")\n",
    "    A = y_target.T.to(torch.float32) @ torch.linalg.pinv((f(X @ layer.gate_proj_weight.T) * (X @ layer.up_proj_weight.T)).to(torch.float32)).T\n",
    "\n",
    "    \n",
    "    print(f\"A shape: {A.shape}\")  # (n, m)\n",
    "    \n",
    "    # Verify the fit\n",
    "    # forward prediction in consistent dtype\n",
    "    X32 = f(X).to(torch.float32)\n",
    "    y32 = y.to(torch.float32)\n",
    "    \n",
    "    y_pred = (A @ ( (f(X @ layer.gate_proj_weight.T) * (X @ layer.up_proj_weight.T)).to(torch.float32) ).T).T + layer.ffn_forward(X)\n",
    "    print(y_pred.shape)\n",
    "    print(y32.shape)\n",
    "    mse = torch.mean((y32 - y_pred) ** 2)\n",
    "    \n",
    "    print(f\"Mean squared error: {mse.item():.6f}\")\n",
    "    print(f)\n",
    "    #print(f\"\\nA =\\n{A}\")\n",
    "    #print(f\"\\nb = {b}\")\n",
    "\n",
    "layer = model_expansion.model.layers[0].mlp\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ebc2da3-9be3-4ac9-bbee-933e4a8e1c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.up_proj_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d131f92a-0fd1-4108-8505-9d3f34a219c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (A @ ( (f(X @ layer.gate_proj_weight.T) * (X @ layer.up_proj_weight.T)).to(torch.float32) ).T).T + layer.ffn_forward(X)\n",
    "\n",
    "y_pred.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d95f044-d77a-4748-9e6e-fcc6fa537397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4199e-08, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y_pred.to('cuda') - layer_clone(X.to('cpu')).to('cuda'))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01f37d3f-4c57-42ff-9f0f-015a3dde1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b7ae3-a959-4e8a-91f0-51faf96c0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "=0.000000014199"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISB (myenv)",
   "language": "python",
   "name": "aisb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
